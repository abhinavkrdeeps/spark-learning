{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "272b265a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/Users/abhinavjha/spark', edit_rc=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e52bc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d62b9c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa453e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b8ae1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe33572",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fcb85aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab371e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac9536e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/27 19:53:00 WARN Utils: Your hostname, Abhinavs-MacBook-Air.local resolves to a loopback address: 127.0.0.1; using 192.168.29.208 instead (on interface en0)\n",
      "24/05/27 19:53:00 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/05/27 19:53:00 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00fbd9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8075ab34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38355f14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1fb78f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "transaction_file = '/Users/abhinavjha/spark-experiments/data/data_skew/transactions.parquet'\n",
    "transaction_df = spark.read.parquet(transaction_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec379585",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6279c274",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be61adb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "transaction_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0078a5ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+---------------+----------+----+-----+---+-------------+------+-----------+\n",
      "|   cust_id|start_date|  end_date|         txn_id|      date|year|month|day| expense_type|   amt|       city|\n",
      "+----------+----------+----------+---------------+----------+----+-----+---+-------------+------+-----------+\n",
      "|C0YDPQWPBJ|2010-07-01|2018-12-01|TZ5SMKZY9S03OQJ|2018-10-07|2018|   10|  7|Entertainment| 10.42|     boston|\n",
      "|C0YDPQWPBJ|2010-07-01|2018-12-01|TYIAPPNU066CJ5R|2016-03-27|2016|    3| 27| Motor/Travel| 44.34|   portland|\n",
      "|C0YDPQWPBJ|2010-07-01|2018-12-01|TETSXIK4BLXHJ6W|2011-04-11|2011|    4| 11|Entertainment|  3.18|    chicago|\n",
      "|C0YDPQWPBJ|2010-07-01|2018-12-01|TQKL1QFJY3EM8LO|2018-02-22|2018|    2| 22|    Groceries|268.97|los_angeles|\n",
      "|C0YDPQWPBJ|2010-07-01|2018-12-01|TYL6DFP09PPXMVB|2010-10-16|2010|   10| 16|Entertainment|  2.66|    chicago|\n",
      "+----------+----------+----------+---------------+----------+----+-----+---+-------------+------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transaction_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c3db0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4827da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e76ed96",
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_file = '/Users/abhinavjha/spark-experiments/data/data_skew/customers.parquet'\n",
    "customers_df = spark.read.parquet(customers_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d13883",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb81e4c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13ce611",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5ccf771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+---+------+----------+-----+-----------+\n",
      "|   cust_id|         name|age|gender|  birthday|  zip|       city|\n",
      "+----------+-------------+---+------+----------+-----+-----------+\n",
      "|C007YEYTX9| Aaron Abbott| 34|Female| 7/13/1991|97823|     boston|\n",
      "|C00B971T1J| Aaron Austin| 37|Female|12/16/2004|30332|    chicago|\n",
      "|C00WRSJF1Q| Aaron Barnes| 29|Female| 3/11/1977|23451|     denver|\n",
      "|C01AZWQMF3|Aaron Barrett| 31|  Male|  7/9/1998|46613|los_angeles|\n",
      "|C01BKUFRHA| Aaron Becker| 54|  Male|11/24/1979|40284|  san_diego|\n",
      "+----------+-------------+---+------+----------+-----+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customers_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494dd615",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdff567b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95bb31e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d382a0f8",
   "metadata": {},
   "source": [
    "## Narrow Transformations (Shuffling not required)\n",
    "\n",
    "1. filter\n",
    "2. Select, add, alter columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b7f259c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+----+------+------+\n",
      "|first_name|last_name| age|gender|  city|\n",
      "+----------+---------+----+------+------+\n",
      "|     Aaron|   Abbott|39.0|Female|boston|\n",
      "|     Aaron|  Lambert|59.0|Female|boston|\n",
      "|     Aaron|  Lindsey|29.0|  Male|boston|\n",
      "|     Aaron|    Lopez|27.0|Female|boston|\n",
      "|     Aaron| Schwartz|57.0|Female|boston|\n",
      "+----------+---------+----+------+------+\n",
      "only showing top 5 rows\n",
      "\n",
      "== Parsed Logical Plan ==\n",
      "'Project ['first_name, 'last_name, 'age, 'gender, 'city]\n",
      "+- Project [cust_id#68, name#69, (cast(age#70 as double) + cast(5 as double)) AS age#364, gender#71, birthday#72, zip#73, city#74, first_name#345, last_name#354]\n",
      "   +- Project [cust_id#68, name#69, age#70, gender#71, birthday#72, zip#73, city#74, first_name#345, split(name#69,  , -1)[1] AS last_name#354]\n",
      "      +- Project [cust_id#68, name#69, age#70, gender#71, birthday#72, zip#73, city#74, split(name#69,  , -1)[0] AS first_name#345]\n",
      "         +- Filter (city#74 = boston)\n",
      "            +- Relation [cust_id#68,name#69,age#70,gender#71,birthday#72,zip#73,city#74] parquet\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "first_name: string, last_name: string, age: double, gender: string, city: string\n",
      "Project [first_name#345, last_name#354, age#364, gender#71, city#74]\n",
      "+- Project [cust_id#68, name#69, (cast(age#70 as double) + cast(5 as double)) AS age#364, gender#71, birthday#72, zip#73, city#74, first_name#345, last_name#354]\n",
      "   +- Project [cust_id#68, name#69, age#70, gender#71, birthday#72, zip#73, city#74, first_name#345, split(name#69,  , -1)[1] AS last_name#354]\n",
      "      +- Project [cust_id#68, name#69, age#70, gender#71, birthday#72, zip#73, city#74, split(name#69,  , -1)[0] AS first_name#345]\n",
      "         +- Filter (city#74 = boston)\n",
      "            +- Relation [cust_id#68,name#69,age#70,gender#71,birthday#72,zip#73,city#74] parquet\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Project [split(name#69,  , -1)[0] AS first_name#345, split(name#69,  , -1)[1] AS last_name#354, (cast(age#70 as double) + 5.0) AS age#364, gender#71, city#74]\n",
      "+- Filter (isnotnull(city#74) AND (city#74 = boston))\n",
      "   +- Relation [cust_id#68,name#69,age#70,gender#71,birthday#72,zip#73,city#74] parquet\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Project [split(name#69,  , -1)[0] AS first_name#345, split(name#69,  , -1)[1] AS last_name#354, (cast(age#70 as double) + 5.0) AS age#364, gender#71, city#74]\n",
      "+- *(1) Filter (isnotnull(city#74) AND (city#74 = boston))\n",
      "   +- *(1) ColumnarToRow\n",
      "      +- FileScan parquet [name#69,age#70,gender#71,city#74] Batched: true, DataFilters: [isnotnull(city#74), (city#74 = boston)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/abhinavjha/spark-experiments/data/data_skew/customers.parq..., PartitionFilters: [], PushedFilters: [IsNotNull(city), EqualTo(city,boston)], ReadSchema: struct<name:string,age:string,gender:string,city:string>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_narrow_transformations = (\n",
    "    customers_df.\n",
    "    filter(F.col(\"city\")==\"boston\")\n",
    "    .withColumn(\"first_name\", F.split(\"name\", \" \").getItem(0))\n",
    "    .withColumn(\"last_name\", F.split(\"name\", \" \").getItem(1))\n",
    "    .withColumn(\"age\", F.col(\"age\")+F.lit(5))\n",
    "    .select([\"first_name\",\"last_name\", \"age\",\"gender\",\"city\"])\n",
    "\n",
    ")\n",
    "\n",
    "df_narrow_transformations.show(5)\n",
    "df_narrow_transformations.explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b72387d",
   "metadata": {},
   "source": [
    "## Wide Transformations (Shuffling is required)\n",
    "1. Repartition\n",
    "2. Coalesce\n",
    "3. joins\n",
    "4. GroupBy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c9610e98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_narrow_transformations.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "34eb8757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "Repartition 12, true\n",
      "+- Project [first_name#345, last_name#354, age#364, gender#71, city#74]\n",
      "   +- Project [cust_id#68, name#69, (cast(age#70 as double) + cast(5 as double)) AS age#364, gender#71, birthday#72, zip#73, city#74, first_name#345, last_name#354]\n",
      "      +- Project [cust_id#68, name#69, age#70, gender#71, birthday#72, zip#73, city#74, first_name#345, split(name#69,  , -1)[1] AS last_name#354]\n",
      "         +- Project [cust_id#68, name#69, age#70, gender#71, birthday#72, zip#73, city#74, split(name#69,  , -1)[0] AS first_name#345]\n",
      "            +- Filter (city#74 = boston)\n",
      "               +- Relation [cust_id#68,name#69,age#70,gender#71,birthday#72,zip#73,city#74] parquet\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "first_name: string, last_name: string, age: double, gender: string, city: string\n",
      "Repartition 12, true\n",
      "+- Project [first_name#345, last_name#354, age#364, gender#71, city#74]\n",
      "   +- Project [cust_id#68, name#69, (cast(age#70 as double) + cast(5 as double)) AS age#364, gender#71, birthday#72, zip#73, city#74, first_name#345, last_name#354]\n",
      "      +- Project [cust_id#68, name#69, age#70, gender#71, birthday#72, zip#73, city#74, first_name#345, split(name#69,  , -1)[1] AS last_name#354]\n",
      "         +- Project [cust_id#68, name#69, age#70, gender#71, birthday#72, zip#73, city#74, split(name#69,  , -1)[0] AS first_name#345]\n",
      "            +- Filter (city#74 = boston)\n",
      "               +- Relation [cust_id#68,name#69,age#70,gender#71,birthday#72,zip#73,city#74] parquet\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Repartition 12, true\n",
      "+- Project [split(name#69,  , -1)[0] AS first_name#345, split(name#69,  , -1)[1] AS last_name#354, (cast(age#70 as double) + 5.0) AS age#364, gender#71, city#74]\n",
      "   +- Filter (isnotnull(city#74) AND (city#74 = boston))\n",
      "      +- Relation [cust_id#68,name#69,age#70,gender#71,birthday#72,zip#73,city#74] parquet\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Exchange RoundRobinPartitioning(12), REPARTITION_BY_NUM, [plan_id=157]\n",
      "   +- Project [split(name#69,  , -1)[0] AS first_name#345, split(name#69,  , -1)[1] AS last_name#354, (cast(age#70 as double) + 5.0) AS age#364, gender#71, city#74]\n",
      "      +- Filter (isnotnull(city#74) AND (city#74 = boston))\n",
      "         +- FileScan parquet [name#69,age#70,gender#71,city#74] Batched: true, DataFilters: [isnotnull(city#74), (city#74 = boston)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/abhinavjha/spark-experiments/data/data_skew/customers.parq..., PartitionFilters: [], PushedFilters: [IsNotNull(city), EqualTo(city,boston)], ReadSchema: struct<name:string,age:string,gender:string,city:string>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df = df_narrow_transformations.repartition(12).explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b13939f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repartition used shuffle to redistribute data across executors. \n",
    "# RoundRobinPartitioning -  Algorithm of performing repartition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a58281d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coalesce - Used to decrease the num of partitions. Difference between df.rdd.repartition and df.rdd.coalesce is\n",
    "# coalesce will always try not to perform shuffle and merge paritions within executors in order to reduce num of partitions.\n",
    "\n",
    "# However, in extreme case coalesce can also perform shuffling in order to meet the num of partitions requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "21ce3048",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "new_df = df_narrow_transformations.repartition(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f6a07f87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f9957be5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "Repartition 5, false\n",
      "+- Repartition 12, true\n",
      "   +- Project [first_name#345, last_name#354, age#364, gender#71, city#74]\n",
      "      +- Project [cust_id#68, name#69, (cast(age#70 as double) + cast(5 as double)) AS age#364, gender#71, birthday#72, zip#73, city#74, first_name#345, last_name#354]\n",
      "         +- Project [cust_id#68, name#69, age#70, gender#71, birthday#72, zip#73, city#74, first_name#345, split(name#69,  , -1)[1] AS last_name#354]\n",
      "            +- Project [cust_id#68, name#69, age#70, gender#71, birthday#72, zip#73, city#74, split(name#69,  , -1)[0] AS first_name#345]\n",
      "               +- Filter (city#74 = boston)\n",
      "                  +- Relation [cust_id#68,name#69,age#70,gender#71,birthday#72,zip#73,city#74] parquet\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "first_name: string, last_name: string, age: double, gender: string, city: string\n",
      "Repartition 5, false\n",
      "+- Repartition 12, true\n",
      "   +- Project [first_name#345, last_name#354, age#364, gender#71, city#74]\n",
      "      +- Project [cust_id#68, name#69, (cast(age#70 as double) + cast(5 as double)) AS age#364, gender#71, birthday#72, zip#73, city#74, first_name#345, last_name#354]\n",
      "         +- Project [cust_id#68, name#69, age#70, gender#71, birthday#72, zip#73, city#74, first_name#345, split(name#69,  , -1)[1] AS last_name#354]\n",
      "            +- Project [cust_id#68, name#69, age#70, gender#71, birthday#72, zip#73, city#74, split(name#69,  , -1)[0] AS first_name#345]\n",
      "               +- Filter (city#74 = boston)\n",
      "                  +- Relation [cust_id#68,name#69,age#70,gender#71,birthday#72,zip#73,city#74] parquet\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Repartition 5, false\n",
      "+- Repartition 12, true\n",
      "   +- Project [split(name#69,  , -1)[0] AS first_name#345, split(name#69,  , -1)[1] AS last_name#354, (cast(age#70 as double) + 5.0) AS age#364, gender#71, city#74]\n",
      "      +- Filter (isnotnull(city#74) AND (city#74 = boston))\n",
      "         +- Relation [cust_id#68,name#69,age#70,gender#71,birthday#72,zip#73,city#74] parquet\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Coalesce 5\n",
      "   +- Exchange RoundRobinPartitioning(12), REPARTITION_BY_NUM, [plan_id=242]\n",
      "      +- Project [split(name#69,  , -1)[0] AS first_name#345, split(name#69,  , -1)[1] AS last_name#354, (cast(age#70 as double) + 5.0) AS age#364, gender#71, city#74]\n",
      "         +- Filter (isnotnull(city#74) AND (city#74 = boston))\n",
      "            +- FileScan parquet [name#69,age#70,gender#71,city#74] Batched: true, DataFilters: [isnotnull(city#74), (city#74 = boston)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/abhinavjha/spark-experiments/data/data_skew/customers.parq..., PartitionFilters: [], PushedFilters: [IsNotNull(city), EqualTo(city,boston)], ReadSchema: struct<name:string,age:string,gender:string,city:string>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df.coalesce(5).explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f3f13cc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10485760b\n"
     ]
    }
   ],
   "source": [
    "# Broadcast join\n",
    "\n",
    "print(spark.conf.get(\"spark.sql.autoBroadcastJoinThreshold\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4fc29c62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "ename": "Py4JError",
     "evalue": "An error occurred while calling o147.executePlan. Trace:\npy4j.Py4JException: Method executePlan([class org.apache.spark.sql.catalyst.plans.logical.Repartition]) does not exist\n\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:321)\n\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:329)\n\tat py4j.Gateway.invoke(Gateway.java:274)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m new_df\u001b[38;5;241m.\u001b[39mcache()\u001b[38;5;241m.\u001b[39mforeach(\u001b[38;5;28;01mlambda\u001b[39;00m x: x) \n\u001b[1;32m      2\u001b[0m catalyst_plan \u001b[38;5;241m=\u001b[39m new_df\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mqueryExecution()\u001b[38;5;241m.\u001b[39mlogical() \n\u001b[0;32m----> 3\u001b[0m spark\u001b[38;5;241m.\u001b[39m_jsparkSession\u001b[38;5;241m.\u001b[39msessionState()\u001b[38;5;241m.\u001b[39mexecutePlan(catalyst_plan)\u001b[38;5;241m.\u001b[39moptimizedPlan()\u001b[38;5;241m.\u001b[39mstats()\u001b[38;5;241m.\u001b[39msizeInBytes()\n",
      "File \u001b[0;32m~/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/spark/python/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:330\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m             \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 330\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m             \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n\u001b[1;32m    333\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    334\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    335\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    336\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name))\n",
      "\u001b[0;31mPy4JError\u001b[0m: An error occurred while calling o147.executePlan. Trace:\npy4j.Py4JException: Method executePlan([class org.apache.spark.sql.catalyst.plans.logical.Repartition]) does not exist\n\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:321)\n\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:329)\n\tat py4j.Gateway.invoke(Gateway.java:274)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\n\n"
     ]
    }
   ],
   "source": [
    "new_df.cache().foreach(lambda x: x) \n",
    "catalyst_plan = new_df._jdf.queryExecution().logical() \n",
    "spark._jsparkSession.sessionState().executePlan(catalyst_plan).optimizedPlan().stats().sizeInBytes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1a11c578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+---+------+----------+-----+-----------+\n",
      "|   cust_id|         name|age|gender|  birthday|  zip|       city|\n",
      "+----------+-------------+---+------+----------+-----+-----------+\n",
      "|C007YEYTX9| Aaron Abbott| 34|Female| 7/13/1991|97823|     boston|\n",
      "|C00B971T1J| Aaron Austin| 37|Female|12/16/2004|30332|    chicago|\n",
      "|C00WRSJF1Q| Aaron Barnes| 29|Female| 3/11/1977|23451|     denver|\n",
      "|C01AZWQMF3|Aaron Barrett| 31|  Male|  7/9/1998|46613|los_angeles|\n",
      "|C01BKUFRHA| Aaron Becker| 54|  Male|11/24/1979|40284|  san_diego|\n",
      "+----------+-------------+---+------+----------+-----+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customers_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bb952127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+---------------+----------+----+-----+---+-------------+------+-----------+\n",
      "|   cust_id|start_date|  end_date|         txn_id|      date|year|month|day| expense_type|   amt|       city|\n",
      "+----------+----------+----------+---------------+----------+----+-----+---+-------------+------+-----------+\n",
      "|C0YDPQWPBJ|2010-07-01|2018-12-01|TZ5SMKZY9S03OQJ|2018-10-07|2018|   10|  7|Entertainment| 10.42|     boston|\n",
      "|C0YDPQWPBJ|2010-07-01|2018-12-01|TYIAPPNU066CJ5R|2016-03-27|2016|    3| 27| Motor/Travel| 44.34|   portland|\n",
      "|C0YDPQWPBJ|2010-07-01|2018-12-01|TETSXIK4BLXHJ6W|2011-04-11|2011|    4| 11|Entertainment|  3.18|    chicago|\n",
      "|C0YDPQWPBJ|2010-07-01|2018-12-01|TQKL1QFJY3EM8LO|2018-02-22|2018|    2| 22|    Groceries|268.97|los_angeles|\n",
      "|C0YDPQWPBJ|2010-07-01|2018-12-01|TYL6DFP09PPXMVB|2010-10-16|2010|   10| 16|Entertainment|  2.66|    chicago|\n",
      "+----------+----------+----------+---------------+----------+----+-----+---+-------------+------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transaction_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "19e5af48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customers_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "74a43f2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39790092"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transaction_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a53584",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "40cc0583",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import broadcast\n",
    "joined_df = transaction_df.join(broadcast(customers_df),how=\"inner\", on=\"cust_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d0d6b3f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Join UsingJoin(Inner, [cust_id])\n",
      ":- Relation [cust_id#0,start_date#1,end_date#2,txn_id#3,date#4,year#5,month#6,day#7,expense_type#8,amt#9,city#10] parquet\n",
      "+- ResolvedHint (strategy=broadcast)\n",
      "   +- Relation [cust_id#68,name#69,age#70,gender#71,birthday#72,zip#73,city#74] parquet\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "cust_id: string, start_date: string, end_date: string, txn_id: string, date: string, year: string, month: string, day: string, expense_type: string, amt: string, city: string, name: string, age: string, gender: string, birthday: string, zip: string, city: string\n",
      "Project [cust_id#0, start_date#1, end_date#2, txn_id#3, date#4, year#5, month#6, day#7, expense_type#8, amt#9, city#10, name#69, age#70, gender#71, birthday#72, zip#73, city#74]\n",
      "+- Join Inner, (cust_id#0 = cust_id#68)\n",
      "   :- Relation [cust_id#0,start_date#1,end_date#2,txn_id#3,date#4,year#5,month#6,day#7,expense_type#8,amt#9,city#10] parquet\n",
      "   +- ResolvedHint (strategy=broadcast)\n",
      "      +- Relation [cust_id#68,name#69,age#70,gender#71,birthday#72,zip#73,city#74] parquet\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Project [cust_id#0, start_date#1, end_date#2, txn_id#3, date#4, year#5, month#6, day#7, expense_type#8, amt#9, city#10, name#69, age#70, gender#71, birthday#72, zip#73, city#74]\n",
      "+- Join Inner, (cust_id#0 = cust_id#68), rightHint=(strategy=broadcast)\n",
      "   :- Filter isnotnull(cust_id#0)\n",
      "   :  +- Relation [cust_id#0,start_date#1,end_date#2,txn_id#3,date#4,year#5,month#6,day#7,expense_type#8,amt#9,city#10] parquet\n",
      "   +- Filter isnotnull(cust_id#68)\n",
      "      +- Relation [cust_id#68,name#69,age#70,gender#71,birthday#72,zip#73,city#74] parquet\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Project [cust_id#0, start_date#1, end_date#2, txn_id#3, date#4, year#5, month#6, day#7, expense_type#8, amt#9, city#10, name#69, age#70, gender#71, birthday#72, zip#73, city#74]\n",
      "   +- BroadcastHashJoin [cust_id#0], [cust_id#68], Inner, BuildRight, false\n",
      "      :- Filter isnotnull(cust_id#0)\n",
      "      :  +- FileScan parquet [cust_id#0,start_date#1,end_date#2,txn_id#3,date#4,year#5,month#6,day#7,expense_type#8,amt#9,city#10] Batched: true, DataFilters: [isnotnull(cust_id#0)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/abhinavjha/spark-experiments/data/data_skew/transactions.p..., PartitionFilters: [], PushedFilters: [IsNotNull(cust_id)], ReadSchema: struct<cust_id:string,start_date:string,end_date:string,txn_id:string,date:string,year:string,mon...\n",
      "      +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, false]),false), [plan_id=392]\n",
      "         +- Filter isnotnull(cust_id#68)\n",
      "            +- FileScan parquet [cust_id#68,name#69,age#70,gender#71,birthday#72,zip#73,city#74] Batched: true, DataFilters: [isnotnull(cust_id#68)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/abhinavjha/spark-experiments/data/data_skew/customers.parq..., PartitionFilters: [], PushedFilters: [IsNotNull(cust_id)], ReadSchema: struct<cust_id:string,name:string,age:string,gender:string,birthday:string,zip:string,city:string>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joined_df.explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b64175d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If one of the dataset is comparatively smaller, BHJ can be used and it can be more optimal. In BHJ, smaller dataset\n",
    "# is hashed and broadcasted to all other nodes where we have partitions of bigger dataset. Then on each of the partitions\n",
    "# join is performed. There is not sort and merge required and hence no shuffling is required\n",
    "\n",
    "# spark.sql.autoBroadcastJoinThreshold\n",
    "# spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ce1468c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle sort merge join\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)\n",
    "spark.conf.set(\"spark.sql.preferSortMergeJoin\", True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ece741ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+---------------+----------+----+-----+---+-------------+------+-----------+--------+---+------+---------+-----+------+\n",
      "|   cust_id|start_date|  end_date|         txn_id|      date|year|month|day| expense_type|   amt|       city|    name|age|gender| birthday|  zip|  city|\n",
      "+----------+----------+----------+---------------+----------+----+-----+---+-------------+------+-----------+--------+---+------+---------+-----+------+\n",
      "|C0YDPQWPBJ|2010-07-01|2018-12-01|TZ5SMKZY9S03OQJ|2018-10-07|2018|   10|  7|Entertainment| 10.42|     boston|Ada Lamb| 32|Female|9/29/2005|22457|denver|\n",
      "|C0YDPQWPBJ|2010-07-01|2018-12-01|TYIAPPNU066CJ5R|2016-03-27|2016|    3| 27| Motor/Travel| 44.34|   portland|Ada Lamb| 32|Female|9/29/2005|22457|denver|\n",
      "|C0YDPQWPBJ|2010-07-01|2018-12-01|TETSXIK4BLXHJ6W|2011-04-11|2011|    4| 11|Entertainment|  3.18|    chicago|Ada Lamb| 32|Female|9/29/2005|22457|denver|\n",
      "|C0YDPQWPBJ|2010-07-01|2018-12-01|TQKL1QFJY3EM8LO|2018-02-22|2018|    2| 22|    Groceries|268.97|los_angeles|Ada Lamb| 32|Female|9/29/2005|22457|denver|\n",
      "|C0YDPQWPBJ|2010-07-01|2018-12-01|TYL6DFP09PPXMVB|2010-10-16|2010|   10| 16|Entertainment|  2.66|    chicago|Ada Lamb| 32|Female|9/29/2005|22457|denver|\n",
      "+----------+----------+----------+---------------+----------+----+-----+---+-------------+------+-----------+--------+---+------+---------+-----+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joined_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b39221ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_merge_join = transaction_df.join(customers_df, how=\"inner\", on=\"cust_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60821f63",
   "metadata": {},
   "source": [
    "## Shuffle Sort Merge Join\n",
    "\n",
    "1. It involves three phases\n",
    "    1.1 Shuffle\n",
    "    1.2 Sort\n",
    "    1.3 Merge\n",
    "\n",
    "- Both datasets are shuffled so that records with same keys ended up in the same partition.\n",
    "- Hash the key to get the partition in which the key will go to. By this, we have records with same keys in the same partition.\n",
    "- After shuffling , records are sorted based on the keys\n",
    "- A join is performed by iterating over the records on the sorted dataset. Since the dataset is sorted the merge or the join operation is stopped for an element as soon as a key mismatch is encountered. So a join attempt is not performed on all keys.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c6bcff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e0102c4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Join UsingJoin(Inner, [cust_id])\n",
      ":- Relation [cust_id#0,start_date#1,end_date#2,txn_id#3,date#4,year#5,month#6,day#7,expense_type#8,amt#9,city#10] parquet\n",
      "+- Relation [cust_id#68,name#69,age#70,gender#71,birthday#72,zip#73,city#74] parquet\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "cust_id: string, start_date: string, end_date: string, txn_id: string, date: string, year: string, month: string, day: string, expense_type: string, amt: string, city: string, name: string, age: string, gender: string, birthday: string, zip: string, city: string\n",
      "Project [cust_id#0, start_date#1, end_date#2, txn_id#3, date#4, year#5, month#6, day#7, expense_type#8, amt#9, city#10, name#69, age#70, gender#71, birthday#72, zip#73, city#74]\n",
      "+- Join Inner, (cust_id#0 = cust_id#68)\n",
      "   :- Relation [cust_id#0,start_date#1,end_date#2,txn_id#3,date#4,year#5,month#6,day#7,expense_type#8,amt#9,city#10] parquet\n",
      "   +- Relation [cust_id#68,name#69,age#70,gender#71,birthday#72,zip#73,city#74] parquet\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Project [cust_id#0, start_date#1, end_date#2, txn_id#3, date#4, year#5, month#6, day#7, expense_type#8, amt#9, city#10, name#69, age#70, gender#71, birthday#72, zip#73, city#74]\n",
      "+- Join Inner, (cust_id#0 = cust_id#68)\n",
      "   :- Filter isnotnull(cust_id#0)\n",
      "   :  +- Relation [cust_id#0,start_date#1,end_date#2,txn_id#3,date#4,year#5,month#6,day#7,expense_type#8,amt#9,city#10] parquet\n",
      "   +- Filter isnotnull(cust_id#68)\n",
      "      +- Relation [cust_id#68,name#69,age#70,gender#71,birthday#72,zip#73,city#74] parquet\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Project [cust_id#0, start_date#1, end_date#2, txn_id#3, date#4, year#5, month#6, day#7, expense_type#8, amt#9, city#10, name#69, age#70, gender#71, birthday#72, zip#73, city#74]\n",
      "   +- SortMergeJoin [cust_id#0], [cust_id#68], Inner\n",
      "      :- Sort [cust_id#0 ASC NULLS FIRST], false, 0\n",
      "      :  +- Exchange hashpartitioning(cust_id#0, 200), ENSURE_REQUIREMENTS, [plan_id=59]\n",
      "      :     +- Filter isnotnull(cust_id#0)\n",
      "      :        +- FileScan parquet [cust_id#0,start_date#1,end_date#2,txn_id#3,date#4,year#5,month#6,day#7,expense_type#8,amt#9,city#10] Batched: true, DataFilters: [isnotnull(cust_id#0)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/abhinavjha/spark-experiments/data/data_skew/transactions.p..., PartitionFilters: [], PushedFilters: [IsNotNull(cust_id)], ReadSchema: struct<cust_id:string,start_date:string,end_date:string,txn_id:string,date:string,year:string,mon...\n",
      "      +- Sort [cust_id#68 ASC NULLS FIRST], false, 0\n",
      "         +- Exchange hashpartitioning(cust_id#68, 200), ENSURE_REQUIREMENTS, [plan_id=60]\n",
      "            +- Filter isnotnull(cust_id#68)\n",
      "               +- FileScan parquet [cust_id#68,name#69,age#70,gender#71,birthday#72,zip#73,city#74] Batched: true, DataFilters: [isnotnull(cust_id#68)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/abhinavjha/spark-experiments/data/data_skew/customers.parq..., PartitionFilters: [], PushedFilters: [IsNotNull(cust_id)], ReadSchema: struct<cust_id:string,name:string,age:string,gender:string,birthday:string,zip:string,city:string>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sort_merge_join.explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24057e5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "81135022",
   "metadata": {},
   "source": [
    "## Group By"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "350a1a77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+---------------+----------+----+-----+---+-------------+------+-----------+\n",
      "|   cust_id|start_date|  end_date|         txn_id|      date|year|month|day| expense_type|   amt|       city|\n",
      "+----------+----------+----------+---------------+----------+----+-----+---+-------------+------+-----------+\n",
      "|C0YDPQWPBJ|2010-07-01|2018-12-01|TZ5SMKZY9S03OQJ|2018-10-07|2018|   10|  7|Entertainment| 10.42|     boston|\n",
      "|C0YDPQWPBJ|2010-07-01|2018-12-01|TYIAPPNU066CJ5R|2016-03-27|2016|    3| 27| Motor/Travel| 44.34|   portland|\n",
      "|C0YDPQWPBJ|2010-07-01|2018-12-01|TETSXIK4BLXHJ6W|2011-04-11|2011|    4| 11|Entertainment|  3.18|    chicago|\n",
      "|C0YDPQWPBJ|2010-07-01|2018-12-01|TQKL1QFJY3EM8LO|2018-02-22|2018|    2| 22|    Groceries|268.97|los_angeles|\n",
      "|C0YDPQWPBJ|2010-07-01|2018-12-01|TYL6DFP09PPXMVB|2010-10-16|2010|   10| 16|Entertainment|  2.66|    chicago|\n",
      "+----------+----------+----------+---------------+----------+----+-----+---+-------------+------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transaction_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09b1c2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "city_count = transaction_df.groupBy(\"city\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ced6b24a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Aggregate ['city], ['city, count(1) AS count#94L]\n",
      "+- Relation [cust_id#0,start_date#1,end_date#2,txn_id#3,date#4,year#5,month#6,day#7,expense_type#8,amt#9,city#10] parquet\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "city: string, count: bigint\n",
      "Aggregate [city#10], [city#10, count(1) AS count#94L]\n",
      "+- Relation [cust_id#0,start_date#1,end_date#2,txn_id#3,date#4,year#5,month#6,day#7,expense_type#8,amt#9,city#10] parquet\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Aggregate [city#10], [city#10, count(1) AS count#94L]\n",
      "+- Project [city#10]\n",
      "   +- Relation [cust_id#0,start_date#1,end_date#2,txn_id#3,date#4,year#5,month#6,day#7,expense_type#8,amt#9,city#10] parquet\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[city#10], functions=[count(1)], output=[city#10, count#94L])\n",
      "   +- Exchange hashpartitioning(city#10, 200), ENSURE_REQUIREMENTS, [plan_id=27]\n",
      "      +- HashAggregate(keys=[city#10], functions=[partial_count(1)], output=[city#10, count#98L])\n",
      "         +- FileScan parquet [city#10] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/abhinavjha/spark-experiments/data/data_skew/transactions.p..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<city:string>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "city_count.explain(True)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a548d198",
   "metadata": {},
   "source": [
    "## HashAggregate and partial count\n",
    "- Local count on each executor. \n",
    "\n",
    "- Exec 1 | Exec 2 | Exec 3\n",
    "\n",
    "  A         B         C\n",
    "  \n",
    "  A         B\n",
    "  \n",
    "  B         C\n",
    "\n",
    "- After HashAggregate and partial count \n",
    "\n",
    "- Exec 1 | Exec 2 | Exec 3\n",
    "\n",
    "  A 2        B 2     C 1\n",
    "            \n",
    "  B 1        C 1\n",
    "  \n",
    " - Shuffle will take place - Exchange hashpartitioning - This will ensure same keys ends up in same partition\n",
    " \n",
    " - Exec 1 | Exec 2 | Exec 3\n",
    " \n",
    "   A 2        B 2     C 1\n",
    "            \n",
    "              B 1     C 1\n",
    "              \n",
    " - After this a final count will happen - HashAggregate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0cd157e",
   "metadata": {},
   "source": [
    "## Aggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "668f796a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+---------------+----------+----+-----+---+-------------+------+-----------+\n",
      "|   cust_id|start_date|  end_date|         txn_id|      date|year|month|day| expense_type|   amt|       city|\n",
      "+----------+----------+----------+---------------+----------+----+-----+---+-------------+------+-----------+\n",
      "|C0YDPQWPBJ|2010-07-01|2018-12-01|TZ5SMKZY9S03OQJ|2018-10-07|2018|   10|  7|Entertainment| 10.42|     boston|\n",
      "|C0YDPQWPBJ|2010-07-01|2018-12-01|TYIAPPNU066CJ5R|2016-03-27|2016|    3| 27| Motor/Travel| 44.34|   portland|\n",
      "|C0YDPQWPBJ|2010-07-01|2018-12-01|TETSXIK4BLXHJ6W|2011-04-11|2011|    4| 11|Entertainment|  3.18|    chicago|\n",
      "|C0YDPQWPBJ|2010-07-01|2018-12-01|TQKL1QFJY3EM8LO|2018-02-22|2018|    2| 22|    Groceries|268.97|los_angeles|\n",
      "|C0YDPQWPBJ|2010-07-01|2018-12-01|TYL6DFP09PPXMVB|2010-10-16|2010|   10| 16|Entertainment|  2.66|    chicago|\n",
      "+----------+----------+----------+---------------+----------+----+-----+---+-------------+------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transaction_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ae131aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "txn_amt_agg = transaction_df.groupby('city').agg(F.sum('amt').alias('txn_amt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "79bead3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:===========================================>              (9 + 3) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+\n",
      "|         city|             txn_amt|\n",
      "+-------------+--------------------+\n",
      "|    san_diego| 3.297982686000007E8|\n",
      "|      chicago|3.2988120044000095E8|\n",
      "|       denver| 3.298814956400023E8|\n",
      "|       boston| 3.301009563300014E8|\n",
      "|      seattle|3.3019513776999813E8|\n",
      "|  los_angeles| 3.303879507600016E8|\n",
      "|     new_york| 3.298224222899999E8|\n",
      "|san_francisco| 3.302435954000013E8|\n",
      "| philadelphia| 3.303539707999984E8|\n",
      "|     portland| 3.309791199400005E8|\n",
      "+-------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "txn_amt_agg.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "04d6ceb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Aggregate ['city], ['city, sum('amt) AS txn_amt#156]\n",
      "+- Relation [cust_id#0,start_date#1,end_date#2,txn_id#3,date#4,year#5,month#6,day#7,expense_type#8,amt#9,city#10] parquet\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "city: string, txn_amt: double\n",
      "Aggregate [city#10], [city#10, sum(cast(amt#9 as double)) AS txn_amt#156]\n",
      "+- Relation [cust_id#0,start_date#1,end_date#2,txn_id#3,date#4,year#5,month#6,day#7,expense_type#8,amt#9,city#10] parquet\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Aggregate [city#10], [city#10, sum(cast(amt#9 as double)) AS txn_amt#156]\n",
      "+- Project [amt#9, city#10]\n",
      "   +- Relation [cust_id#0,start_date#1,end_date#2,txn_id#3,date#4,year#5,month#6,day#7,expense_type#8,amt#9,city#10] parquet\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[city#10], functions=[sum(cast(amt#9 as double))], output=[city#10, txn_amt#156])\n",
      "   +- Exchange hashpartitioning(city#10, 200), ENSURE_REQUIREMENTS, [plan_id=106]\n",
      "      +- HashAggregate(keys=[city#10], functions=[partial_sum(cast(amt#9 as double))], output=[city#10, sum#166])\n",
      "         +- FileScan parquet [amt#9,city#10] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/abhinavjha/spark-experiments/data/data_skew/transactions.p..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<amt:string,city:string>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "txn_amt_agg.explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f47676",
   "metadata": {},
   "source": [
    "- functions=[sum(cast(amt#9 as double))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aefe336",
   "metadata": {},
   "source": [
    "## count distinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1fe73ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "city_count_df = transaction_df.groupby('cust_id').agg(F.countDistinct('city').alias('city_count'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d298568",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "29d6e143",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 10:==============================================>         (10 + 2) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|   cust_id|city_count|\n",
      "+----------+----------+\n",
      "|CPP8BY8U93|        10|\n",
      "|CYB8BX9LU1|        10|\n",
      "|CFRT841CCD|        10|\n",
      "|CA0TSNMYDK|        10|\n",
      "|COZ8NONEVZ|        10|\n",
      "|C46OCVH3WG|        10|\n",
      "|C1QF29WCA6|        10|\n",
      "|CTJBQB0OJ1|        10|\n",
      "|CD0DXL8XTM|        10|\n",
      "|CADBQ5OL5C|        10|\n",
      "|CUCQ9LBQWW|        10|\n",
      "|C3NH8CDGWM|        10|\n",
      "|CEEPXNQ9NQ|        10|\n",
      "|C7ALJDG81A|        10|\n",
      "|CUDKFKPAFB|        10|\n",
      "|C2L2984OZK|        10|\n",
      "|CDDRDAEY13|        10|\n",
      "|CIZT509YVA|        10|\n",
      "|CSTJ6YYXE3|        10|\n",
      "|CW1X1V0PRG|        10|\n",
      "+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "city_count_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8ed43913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Aggregate ['cust_id], ['cust_id, 'count(distinct 'city) AS city_count#211]\n",
      "+- Relation [cust_id#0,start_date#1,end_date#2,txn_id#3,date#4,year#5,month#6,day#7,expense_type#8,amt#9,city#10] parquet\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "cust_id: string, city_count: bigint\n",
      "Aggregate [cust_id#0], [cust_id#0, count(distinct city#10) AS city_count#211L]\n",
      "+- Relation [cust_id#0,start_date#1,end_date#2,txn_id#3,date#4,year#5,month#6,day#7,expense_type#8,amt#9,city#10] parquet\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Aggregate [cust_id#0], [cust_id#0, count(distinct city#10) AS city_count#211L]\n",
      "+- Project [cust_id#0, city#10]\n",
      "   +- Relation [cust_id#0,start_date#1,end_date#2,txn_id#3,date#4,year#5,month#6,day#7,expense_type#8,amt#9,city#10] parquet\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[cust_id#0], functions=[count(distinct city#10)], output=[cust_id#0, city_count#211L])\n",
      "   +- Exchange hashpartitioning(cust_id#0, 200), ENSURE_REQUIREMENTS, [plan_id=292]\n",
      "      +- HashAggregate(keys=[cust_id#0], functions=[partial_count(distinct city#10)], output=[cust_id#0, count#223L])\n",
      "         +- HashAggregate(keys=[cust_id#0, city#10], functions=[], output=[cust_id#0, city#10])\n",
      "            +- Exchange hashpartitioning(cust_id#0, city#10, 200), ENSURE_REQUIREMENTS, [plan_id=288]\n",
      "               +- HashAggregate(keys=[cust_id#0, city#10], functions=[], output=[cust_id#0, city#10])\n",
      "                  +- FileScan parquet [cust_id#0,city#10] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/abhinavjha/spark-experiments/data/data_skew/transactions.p..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<cust_id:string,city:string>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "city_count_df.explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400fb2a6",
   "metadata": {},
   "source": [
    "### Why is a filter step present despite predicate pushdown?\n",
    "- This is largely due to the way Spark's Catalyst Optimizer works. Specifically, due to two separate stages of the query optimization process: Physical Planning and Logical Planning.\n",
    "\n",
    "Logical Planning: Catalyst optimizer simplifies the unresolved logical plan (which represents the user's query) by applying various rule-based optimizations. This includes predicate pushdown, projection pushdown where filter conditions and column projections are moved as close to the data source as possible.\n",
    "\n",
    "Physical Planning phase is where the logical plan is translated into one or more physical plans, which can actually be executed on the cluster. This includes operations like file scans, filters, projections, etc.\n",
    "\n",
    "In this case, during the logical planning phase, the predicate (F.col(\"city\") == \"boston\") has been pushed down and will be applied during the scan of the Parquet file (PushedFilters: [IsNotNull(city), EqualTo(city,boston)]), thus improving performance.\n",
    "\n",
    "Now, during the physical planning phase, the same filter condition (+- *(1) Filter (isnotnull(city#73) AND (city#73 = boston))) is applied again to the data that's been loaded into memory. This is because of the following reasons:\n",
    "\n",
    "Guaranteed Correctness: It might seem redundant, but remember that not all data sources can handle pushed-down predicates, and not all predicates can be pushed down. Therefore, even if a predicate is pushed down to the data source, Spark still includes the predicate in the physical plan to cover cases where the data source might not have been able to fully apply the predicate. This is Spark's way of making sure the correct data is always returned, no matter the capabilities of the data source.\n",
    "\n",
    "No Assumptions: Spark's Catalyst optimizer doesn't make assumptions about the data source's ability to handle pushed-down predicates. The optimizer aims to generate plans that return correct results across a wide range of scenarios. Even if the filter is pushed down, Spark does not have the feedback from data source whether the pushdown was successful or not, so it includes the filter operation in the physical plan as well.\n",
    "\n",
    "It is more of a fail-safe mechanism to ensure data integrity and correctness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3acadae8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "75898d22",
   "metadata": {},
   "source": [
    "In what cases will predicate pushdown not work?\n",
    "\n",
    "2 Examples where filter pushdown will not work:\n",
    "\n",
    "Complex Data Types: Spark's Parquet data source does not push down filters that involve complex types, such as arrays, maps, and structs. This is because these complex data types can have complicated nested structures that the Parquet reader cannot easily filter on.\n",
    "Here's an example:\n",
    "\n",
    "root\n",
    " |-- Name: string (nullable = true)\n",
    " |-- properties: map (nullable = true)\n",
    " |    |-- key: string\n",
    " |    |-- value: string (valueContainsNull = true)\n",
    "\n",
    "+----------+-----------------------------+\n",
    "|Name      |properties                   |\n",
    "+----------+-----------------------------+\n",
    "|Afaque    |[eye -> black, hair -> black]|\n",
    "|Naved     |[eye ->, hair -> brown]      |\n",
    "|Ali       |[eye -> black, hair -> red]  |\n",
    "|Amaan     |[eye -> grey, hair -> grey]  |\n",
    "|Omaira    |[eye -> , hair -> brown]     |\n",
    "+----------+-----------------------------+\n",
    "df.filter(df.properties.getItem(\"eye\") == \"brown\").show()\n",
    "== Physical Plan ==\n",
    "*(1) Filter (metadata#123[key] = value)\n",
    "+- *(1) ColumnarToRow\n",
    "   +- FileScan parquet [id#122,metadata#123] Batched: true, DataFilters: [(metadata#123[key] = value)], Format: Parquet, ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac15ce12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5bfb66a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a48463",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee289e00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96355d23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209a8998",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc278bd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea359258",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9036108c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed328d7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60136d18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e7beb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d95c02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736fff73",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
